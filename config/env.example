# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âš ï¸  TEMPLATE FILE - DO NOT SOURCE DIRECTLY âš ï¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# This is a template configuration file with placeholder values.
# Copy this file to .env and replace placeholders with your actual values:
#
#   cp .env.example .env
#   # Then edit .env with your real API keys
#
# IMPORTANT: If your shell auto-loads .env files, make sure it EXCLUDES .env.example
# to prevent placeholder values from overwriting your real configuration!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SEMANTIC VARIABLE NAMES (RECOMMENDED - USE THESE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# These variable names clearly represent what they actually do.
# Use these instead of the legacy names below for clarity and correctness.

# Required: Your API key for your backend provider
# Works with ANY OpenAI-compatible provider:
# - OpenRouter: sk-or-v1-YOUR-KEY-HERE
# - OpenAI: sk-YOUR-KEY-HERE
# - Azure: your-azure-key
# - Local models: any value (or "dummy-key")
PROVIDER_API_KEY="sk-your-provider-api-key-here"

# Optional: API base URL for your backend provider (default: https://api.openai.com/v1)
# Common examples:
# - OpenRouter: https://openrouter.ai/api/v1
# - OpenAI: https://api.openai.com/v1
# - Azure: https://your-resource.openai.azure.com/openai/deployments/your-deployment-name
# - Local (Ollama): http://localhost:11434/v1
# - Local (LM Studio): http://localhost:1234/v1
PROVIDER_BASE_URL="https://api.openai.com/v1"

# Optional: Authentication key for proxy clients
# If set, clients must provide this exact key to access the proxy
# This is NOT for Anthropic's API - it's for securing YOUR proxy
PROXY_AUTH_KEY="your-proxy-auth-key"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEGACY VARIABLE NAMES (DEPRECATED - DO NOT USE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# These variable names are misleading and deprecated. They still work for backward
# compatibility but will show deprecation warnings. Use the semantic names above.
#
# OPENAI_API_KEY â†’ Use PROVIDER_API_KEY (this key is for ANY provider, not just OpenAI)
# OPENAI_BASE_URL â†’ Use PROVIDER_BASE_URL
# ANTHROPIC_API_KEY â†’ Use PROXY_AUTH_KEY (this is for proxy auth, NOT Anthropic's API)
#
# DO NOT SET THESE - USE THE SEMANTIC NAMES ABOVE INSTEAD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Optional: Model mappings (BIG, MIDDLE, and SMALL models)
# BIG_MODEL="gpt-4o"
# Used for Claude opus requests
# MIDDLE_MODEL="gpt-4o"
# Used for Claude sonnet requests
# SMALL_MODEL="gpt-4o-mini"
# Used for Claude haiku requests

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REASONING CONFIGURATION - Advanced reasoning for OpenAI, Anthropic, and Gemini
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global reasoning effort level for OpenAI o-series models (o1, o3, o4-mini, gpt-5)
# Valid values: "low", "medium", "high"
# Default: not set (reasoning disabled unless explicitly enabled)
# This setting applies to all reasoning-capable models unless overridden by model suffix
# Example: REASONING_EFFORT="high"
REASONING_EFFORT=""

# Maximum tokens for reasoning/thinking (Anthropic Claude and Google Gemini)
# Anthropic Claude (3.7, 4.x): Valid range 1024-128000 tokens
# Google Gemini (2.5-flash): Valid range 0-24576 tokens
# Default: not set (uses provider default)
# Example: REASONING_MAX_TOKENS="128000"
REASONING_MAX_TOKENS=""

# Whether to exclude reasoning tokens from the response
# Set to "true" to hide reasoning tokens in the response (OpenAI only)
# Default: "false" (include reasoning tokens)
REASONING_EXCLUDE="false"

# Verbosity setting for responses (affects detail level)
# Provider-specific setting for response verbosity
# Default: not set
# Example: VERBOSITY="high"
VERBOSITY=""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USAGE TRACKING & ANALYTICS (Optional)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable persistent API usage tracking (opt-in feature)
# When enabled, stores request metadata in SQLite for analytics
# Does NOT store message content - only metadata (tokens, cost, performance)
# Data stored: model, tokens, duration, cost estimates, request counts
# Privacy: All tracking is local, no data sent anywhere
# Default: "false" (disabled)
TRACK_USAGE="false"

# Usage database location
# Default: "usage_tracking.db" in project root
# USAGE_DB_PATH="usage_tracking.db"

# Enable compact single-line logging format
# Uses emojis and sophisticated color coding for terminal output
# Alternative to the default multi-line format
# Default: "false" (uses standard logger)
COMPACT_LOGGER="false"

# Enable live terminal dashboard
# Shows real-time metrics, active requests, and performance stats
# Default: "false"
ENABLE_DASHBOARD="false"

# Dashboard layout style
# Options: "default", "compact", "detailed"
# Default: "default"
DASHBOARD_LAYOUT="default"

# Dashboard refresh rate in seconds
# Default: "0.5"
DASHBOARD_REFRESH="0.5"

# Number of recent requests to show in waterfall view
# Default: "20"
DASHBOARD_WATERFALL_SIZE="20"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PER-MODEL REASONING OVERRIDES (Optional)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Override reasoning effort for specific model sizes
# These override the global REASONING_EFFORT setting for each model
# Valid values: "low", "medium", "high"
# Only applies to reasoning-capable models (OpenAI o-series)

# BIG model reasoning override (Claude Opus â†’ configured BIG_MODEL)
# Example: BIG_MODEL_REASONING="high"
BIG_MODEL_REASONING=""

# MIDDLE model reasoning override (Claude Sonnet â†’ configured MIDDLE_MODEL)
# Example: MIDDLE_MODEL_REASONING="medium"
MIDDLE_MODEL_REASONING=""

# SMALL model reasoning override (Claude Haiku â†’ configured SMALL_MODEL)
# Example: SMALL_MODEL_REASONING="low"
SMALL_MODEL_REASONING=""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REASONING EXAMPLES - Model name suffix notation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# You can also specify reasoning parameters directly in model names using suffix notation:
#
# OpenAI o-series (effort levels OR arbitrary token budgets):
#   - "o4-mini:low"      â†’ Low reasoning effort
#   - "o4-mini:medium"   â†’ Medium reasoning effort
#   - "o4-mini:high"     â†’ High reasoning effort
#   - "o4-mini:50000"    â†’ 50,000 thinking tokens (arbitrary budget)
#   - "o4-mini:350000"   â†’ 350,000 thinking tokens (for 400k context)
#   - "o4-mini:100k"     â†’ 102,400 thinking tokens (k-notation)
#
# Anthropic Claude (thinking tokens):
#   - "claude-opus-4-20250514:1k"    â†’ 1024 thinking tokens
#   - "claude-opus-4-20250514:4k"    â†’ 4096 thinking tokens
#   - "claude-opus-4-20250514:8k"    â†’ 8192 thinking tokens
#   - "claude-opus-4-20250514:16k"   â†’ 16384 thinking tokens
#   - "claude-opus-4-20250514:8000"  â†’ 8000 thinking tokens (exact number)
#
# Google Gemini (thinking budget):
#   - "gemini-2.5-flash-preview-04-17:4k"     â†’ 4096 thinking budget
#   - "gemini-2.5-flash-preview-04-17:16000"  â†’ 16000 thinking budget
#
# Suffix notation overrides environment variable defaults
# Terminal output shows: model routing, endpoint, thinking tokens used vs budget

# Optional: Enable/disable OpenRouter model selection in interactive selector
# Set to "false" to hide OpenRouter marketplace models (only show local models)
# Default: "true" (show all models including OpenRouter)
# Example: ENABLE_OPENROUTER_SELECTION="false"
ENABLE_OPENROUTER_SELECTION="true"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYBRID MODE - Optional per-model routing (mix local and remote models!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Enable per-model routing for BIG model (Claude Opus)
# Set to "true" to enable custom endpoint for BIG model
# Example: ENABLE_BIG_ENDPOINT="true"
ENABLE_BIG_ENDPOINT="false"

# BIG model endpoint URL (if enabled above)
# Example: BIG_ENDPOINT="http://localhost:11434/v1"
BIG_ENDPOINT="https://api.openai.com/v1"

# BIG model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: BIG_API_KEY="sk-or-..."
BIG_API_KEY=""

# Enable per-model routing for MIDDLE model (Claude Sonnet)
# Set to "true" to enable custom endpoint for MIDDLE model
# Example: ENABLE_MIDDLE_ENDPOINT="true"
ENABLE_MIDDLE_ENDPOINT="false"

# MIDDLE model endpoint URL (if enabled above)
# Example: MIDDLE_ENDPOINT="https://openrouter.ai/api/v1"
MIDDLE_ENDPOINT="https://api.openai.com/v1"

# MIDDLE model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: MIDDLE_API_KEY="sk-or-..."
MIDDLE_API_KEY=""

# Enable per-model routing for SMALL model (Claude Haiku)
# Set to "true" to enable custom endpoint for SMALL model
# Example: ENABLE_SMALL_ENDPOINT="true"
ENABLE_SMALL_ENDPOINT="false"

# SMALL model endpoint URL (if enabled above)
# Example: SMALL_ENDPOINT="http://127.0.0.1:1234/v1"
SMALL_ENDPOINT="https://api.openai.com/v1"

# SMALL model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: SMALL_API_KEY="sk-or-..."
SMALL_API_KEY=""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM SYSTEM PROMPTS (Optional)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Override system prompts for specific model tiers
# Useful for customizing behavior per model size

# Enable custom prompts per model
ENABLE_CUSTOM_BIG_PROMPT="false"
ENABLE_CUSTOM_MIDDLE_PROMPT="false"
ENABLE_CUSTOM_SMALL_PROMPT="false"

# System prompt from file (takes precedence if both file and inline are set)
# BIG_SYSTEM_PROMPT_FILE="/path/to/big_prompt.txt"
# MIDDLE_SYSTEM_PROMPT_FILE="/path/to/middle_prompt.txt"
# SMALL_SYSTEM_PROMPT_FILE="/path/to/small_prompt.txt"

# System prompt inline (used if no file is specified)
# BIG_SYSTEM_PROMPT="You are a helpful assistant specialized in..."
# MIDDLE_SYSTEM_PROMPT="You are a helpful assistant..."
# SMALL_SYSTEM_PROMPT="You are a helpful assistant..."

# Optional: Server settings
HOST="0.0.0.0"
PORT="8082"
LOG_LEVEL="INFO"
# DEBUG, INFO, WARNING, ERROR, CRITICAL

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WEB UI CONFIGURATION (Optional)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# The Web UI is automatically available at http://localhost:8082/ or http://localhost:8082/config
# Features:
# - Visual configuration editor (no restart required!)
# - Profile management (save/load/delete different configs)
# - Model browser with search and filtering
# - Real-time monitoring and statistics
# - Usage analytics (when TRACK_USAGE is enabled)
#
# No additional configuration needed - just access http://localhost:8082 in your browser!

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TERMINAL OUTPUT CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Terminal display mode
# Options: "minimal", "normal", "detailed", "debug"
# Default: "detailed"
TERMINAL_DISPLAY_MODE="detailed"

# Show workspace/project name in terminal output
# Default: "true"
TERMINAL_SHOW_WORKSPACE="true"

# Show context window percentage
# Default: "true"
TERMINAL_SHOW_CONTEXT_PCT="true"

# Show task type icons (ğŸ§  REASON, ğŸ”§ TOOLS, ğŸ“ TEXT, ğŸ–¼ï¸ IMAGE)
# Default: "true"
TERMINAL_SHOW_TASK_TYPE="true"

# Show tokens per second (performance metric)
# Default: "true"
TERMINAL_SHOW_SPEED="true"

# Show cost estimates per request
# Default: "true"
TERMINAL_SHOW_COST="true"

# Enable color-coded request durations (fast=green, medium=yellow, slow=red)
# Default: "true"
TERMINAL_SHOW_DURATION_COLORS="true"

# Color scheme for terminal output
# Options: "auto", "vibrant", "subtle", "mono"
# Default: "auto"
TERMINAL_COLOR_SCHEME="auto"

# Enable session color differentiation (different color per Claude Code session)
# Default: "true"
TERMINAL_SESSION_COLORS="true"

# Log style: "rich" (colored with progress bars), "plain" (no colors), "compact" (minimal)
# Default: "rich"
LOG_STYLE="rich"

# Show token counts in logs (input/output/thinking tokens)
# Default: "true"
SHOW_TOKEN_COUNTS="true"

# Show performance metrics (tokens/sec, latency)
# Default: "true"
SHOW_PERFORMANCE="true"

# Color scheme: "auto" (detect terminal), "dark", "light", "none" (disable colors)
# Default: "auto"
COLOR_SCHEME="auto"

# Optional: Performance settings  
MAX_TOKENS_LIMIT="4096"
# Minimum tokens limit for requests (to avoid errors with thinking model)
MIN_TOKENS_LIMIT="4096"
REQUEST_TIMEOUT="90"
MAX_RETRIES="2"

# Examples for other providers:

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VIBEPROXY / ANTIGRAVITY INTEGRATION (Claude & Gemini via OAuth)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# VibeProxy is a macOS menu bar app that wraps CLIProxyAPI, providing OAuth
# authentication to Antigravity (Google's Claude/Gemini service), Gemini CLI,
# Claude Code, and other providers.
#
# WHY USE VIBEPROXY + THIS PROXY?
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VibeProxy alone just forwards requests. This proxy adds:
# - BIG/MIDDLE/SMALL model routing (opusâ†’BIG, sonnetâ†’MIDDLE, haikuâ†’SMALL)
# - Extended thinking budget control (up to 128,000 tokens)
# - Usage tracking and analytics
# - Terminal dashboard with real-time metrics
# - Hybrid mode (mix local + cloud models)
# - Custom system prompts per model tier
# - Web UI for configuration
#
# WHAT IS ANTIGRAVITY?
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Antigravity is Google's AI service that provides access to:
# - Claude models (Opus, Sonnet) via Google's infrastructure
# - Gemini models (3-flash, 3-pro, 2.5-flash)
# - Extended thinking support for Claude models
# Antigravity uses OAuth authentication (handled by VibeProxy).
#
# SETUP:
# â”€â”€â”€â”€â”€â”€
# 1. Install VibeProxy: https://github.com/automazeio/vibeproxy/releases
# 2. Launch VibeProxy and authenticate with Antigravity
# 3. Run: python start_proxy.py --setup (select VibeProxy option)
#    Or manually configure:
#
# PROVIDER_API_KEY="dummy"  # Not used - VibeProxy handles OAuth
# PROVIDER_BASE_URL="http://127.0.0.1:8317/v1"
#
# BIG_MODEL="gemini-claude-opus-4-5-thinking"     # Claude Opus with 128k thinking
# MIDDLE_MODEL="gemini-3-pro-preview"             # Fast Gemini 3 Pro
# SMALL_MODEL="gemini-3-flash"                    # Ultra-fast Gemini 3 Flash
# REASONING_MAX_TOKENS="128000"                   # Max thinking tokens
#
# AVAILABLE ANTIGRAVITY MODELS (via VibeProxy):
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini Models:
#   gemini-3-flash                    # Fast, efficient
#   gemini-3-pro-preview              # Powerful, balanced
#   gemini-3-pro-image-preview        # With vision capabilities
#   gemini-2.5-flash                  # Stable previous gen
#   gemini-2.5-flash-lite             # Lightweight variant
#
# Claude Models (via Antigravity):
#   gemini-claude-sonnet-4-5          # Claude Sonnet
#   gemini-claude-sonnet-4-5-thinking # Claude Sonnet with extended thinking
#   gemini-claude-opus-4-5-thinking   # Claude Opus with extended thinking (128k)
#
# Other:
#   gpt-oss-120b-medium               # Open source GPT model
#
# To check available models: curl -s http://127.0.0.1:8317/v1/models | jq '.data[].id'
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# For Azure OpenAI (recommended if OpenAI is not available in your region):
# OPENAI_API_KEY="your-azure-api-key"
# OPENAI_BASE_URL="https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name"
# AZURE_API_VERSION="2024-03-01-preview"
# BIG_MODEL="gpt-4"
# MIDDLE_MODEL="gpt-4"
# SMALL_MODEL="gpt-35-turbo"

# For local models (like Ollama):
# OPENAI_API_KEY="dummy-key"  # Required but can be any value for local models
# OPENAI_BASE_URL="http://localhost:11434/v1"
# BIG_MODEL="llama3.1:70b"
# MIDDLE_MODEL="llama3.1:70b"
# SMALL_MODEL="llama3.1:8b"

# Note: If you get "unsupported_country_region_territory" errors,
# consider using Azure OpenAI or a local model setup instead.


# Custom Headers Configuration
# Format: HEADER_KEY=header_value
# These headers will be automatically included in API requests
# Uncomment the lines below to use custom headers:
# CUSTOM_HEADER_ACCEPT="application/jsonstream"
# CUSTOM_HEADER_CONTENT_TYPE="application/json"
# CUSTOM_HEADER_USER_AGENT="node-fetch"
# CUSTOM_HEADER_HOST="example.com"
# CUSTOM_HEADER_AUTHORIZATION="Bearer your-token"
# CUSTOM_HEADER_X_API_KEY="your-api-key"
# CUSTOM_HEADER_X_CLIENT_ID="your-client-id"
