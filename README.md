<div align="center">

# Claude Code Proxy

**Use Claude Code with any OpenAI-compatible provider**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688.svg)](https://fastapi.tiangolo.com/)

</div>

---

## üéØ What is this?

Claude Code Proxy is a **drop-in replacement** that enables the official Claude Code CLI to work with **any OpenAI-compatible API provider** (OpenAI, OpenRouter, Azure OpenAI, local models via Ollama/LMStudio, and more).

### Why use it?

- ‚úÖ **Use Claude Code with any provider** - Not just Anthropic's API
- ‚úÖ **Save money** - Use local models (Ollama/LMStudio) or cheaper providers
- ‚úÖ **Advanced Reasoning** - Arbitrary thinking token budgets (50k, 350k, etc.)
- ‚úÖ **Rich Terminal Output** - Color-coded logs with progress bars
- ‚úÖ **Token Visualization** - See context window and output usage in real-time
- ‚úÖ **352+ models** - Browse and configure from our model database
- ‚úÖ **Smart templates** - One-click setups for common use cases
- ‚úÖ **Performance Metrics** - Tokens/sec, latency, thinking token tracking
- ‚úÖ **Local models** - Run everything on your machine

- **Full Claude API Compatibility**: Complete `/v1/messages` endpoint support
- **Multiple Provider Support**: OpenAI, Azure OpenAI, local models (Ollama), and any OpenAI-compatible API
- **Smart Model Mapping**: Configure BIG and SMALL models via environment variables
- **Function Calling**: Complete tool use support with proper conversion
- **Streaming Responses**: Real-time SSE streaming support
- **Image Support**: Base64 encoded image input
- **Custom Headers**: Automatic injection of custom HTTP headers for API requests
- **Error Handling**: Comprehensive error handling and logging

## üöÄ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/yourusername/claude-code-proxy.git
cd claude-code-proxy

# Using UV (recommended)
uv sync

# Or using pip
pip install -r requirements.txt
```

### 2. Configure

```bash
cp .env.example .env
# Edit .env with your settings
```

> **‚ö†Ô∏è IMPORTANT: New Semantic Variable Names**
>
> We've introduced clearer, semantic variable names to eliminate confusion:
> - `PROVIDER_API_KEY` ‚Üí Your backend provider's API key (replaces `OPENAI_API_KEY`)
> - `PROVIDER_BASE_URL` ‚Üí Your backend provider's URL (replaces `OPENAI_BASE_URL`)
> - `PROXY_AUTH_KEY` ‚Üí Proxy client authentication (replaces `ANTHROPIC_API_KEY`)
>
> **Legacy names still work** but show deprecation warnings. Use the new semantic names!

**Example: OpenRouter with GPT-5 (High Reasoning)**
```bash
PROVIDER_API_KEY="your-openrouter-key"
PROVIDER_BASE_URL="https://openrouter.ai/api/v1"
BIG_MODEL="openai/gpt-5"
REASONING_EFFORT="high"
REASONING_MAX_TOKENS="8000"
VERBOSITY="high"
REASONING_EXCLUDE="false"
```

**Example: Anthropic Claude with Reasoning**
```bash
PROVIDER_API_KEY="your-openrouter-key"
PROVIDER_BASE_URL="https://openrouter.ai/api/v1"
BIG_MODEL="anthropic/claude-4.1-sonnet"
REASONING_EFFORT="high"
REASONING_MAX_TOKENS="16000"
REASONING_EXCLUDE="false"
```

**Example: Free Local Models (No API costs!)**
```bash
BIG_MODEL="ollama/qwen2.5:72b"
MIDDLE_MODEL="ollama/llama3.1:70b"
SMALL_MODEL="ollama/llama3.1:8b"
REASONING_EFFORT="medium"
ENABLE_OPENROUTER_SELECTION="false"  # Hide marketplace, show only local
```

**Example: Local-Only Deployment**
```bash
# Hide all OpenRouter models from selector
ENABLE_OPENROUTER_SELECTION="false"

# Use only local models
BIG_MODEL="ollama/qwen2.5:72b"
MIDDLE_MODEL="ollama/llama3.1:70b"
SMALL_MODEL="lmstudio/Meta-Llama-3.1-8B-Instruct"
REASONING_EFFORT="medium"
```

### 3. Start Server

**Option A: Direct Python**
```bash
python start_proxy.py
```

**Option B: Docker (Recommended for Production)**
```bash
# Build and run with Docker Compose
docker-compose up --build

# Or build and run manually
docker build -t claude-code-proxy .
docker run -p 8082:8082 --env-file .env claude-code-proxy
```

### 4. Use with Claude Code

```bash
# Set the base URL
export ANTHROPIC_BASE_URL=http://localhost:8082

# Use Claude Code normally
claude "Write a Python function to calculate fibonacci numbers"
```

---

## ‚ú® Features

### Core Features

| Feature | Description |
|---------|-------------|
| **Full Claude API Compatibility** | Complete `/v1/messages` endpoint support with all Claude features |
| **Multi-Provider Support** | OpenAI, Azure, OpenRouter, local (Ollama/LMStudio), any OpenAI-compatible API |
| **Smart Model Mapping** | Configure BIG, MIDDLE, SMALL models via environment variables |
| **Function Calling** | Full tool/function calling support with proper conversion |
| **Streaming Responses** | Real-time Server-Sent Events (SSE) streaming |
| **Image Support** | Handle base64 encoded image inputs |
| **Error Handling** | Comprehensive error handling with detailed logging |

### Advanced Features

#### ü§ñ GPT-5 & Advanced Reasoning

Full support for reasoning-capable models across all providers via OpenRouter's unified API:

```bash
REASONING_EFFORT="high"        # low, medium, high
REASONING_MAX_TOKENS="8000"    # Optional: fine-grained token control
VERBOSITY="high"               # Response detail level
REASONING_EXCLUDE="false"      # Show/hide reasoning tokens
```

**Comprehensive reasoning support:**

**OpenAI Models:**
- GPT-5 family (openai/gpt-5)
- o1 series (openai/o1, openai/o1-mini)
- o3 series (openai/o3, openai/o3-mini)

**Anthropic Models:**
- Claude 3.7 Sonnet/Opus/Haiku
- Claude 4.x series
- Claude 4.1 series

**OpenRouter & Other Providers:**
- Qwen3 and Qwen-2.5 thinking models
- DeepSeek V3, V3.1, and R1 variants
- xAI Grok with reasoning
- MiniMax M2 thinking models
- Kimi K2 thinking models
- Local models (Ollama Qwen 2.5, DeepSeek V2.5)

**Unified Reasoning Control:**
- Uses OpenRouter's `reasoning` object for cross-provider compatibility
- Supports both `effort` (OpenAI-style) and `max_tokens` (Anthropic-style)
- Auto-detects reasoning-capable models from model metadata
- Preserves reasoning blocks in responses for tool-calling workflows

#### üì¶ Mode Templates (10 Pre-Built Configurations)

One-click setups for common use cases:

1. **Free Tier** - Completely free local models
2. **Development** - Cost-effective setup
3. **Production** - Premium GPT-5 setup
4. **Reasoning Intensive** - For complex analysis
5. **Vision & Multimodal** - Image processing
6. **Fast Inference** - Speed optimized
7. **Local Reasoning** - Free local with reasoning
8. **Budget Conscious** - Ultra-low cost
9. **LMStudio** - GUI-based local models
10. **Research** - High context models

#### üéØ Smart Model Recommender

Intelligent suggestions based on your usage:

- **Find free alternatives** to expensive models
- **Usage pattern analysis** - see what works for you
- **Correlation tracking** - find models used together
- **Cost optimization** - discover cheaper alternatives

#### üìä Model Database

**352 models** from multiple sources:

- **OpenRouter** - 340+ models
- **LMStudio** - 5 local models (port 1234)
- **Ollama** - 7 local models (port 11434)
- **140 free models** - All listed and easy to find

**Detailed metadata for each model:**
- Pricing per 1M tokens
- Context window size
- Reasoning support
- Vision support
- Provider information
- Endpoint URLs

#### üíæ Configuration Modes

Save and load different configurations:

- **99 mode slots** (ID 1-99)
- **Load by name or ID**
- **Case-insensitive lookup**
- **JSON persistence**
- **Export/import ready**

#### üé® Interactive Selector

Beautiful terminal UI with:

- **Color-coded interface** - 16-color palette
- **ASCII art headers** - Professional appearance
- **Browse by capability** - Filter by reasoning, vision, free, local
- **Search functionality** - Find specific models
- **Local models filter** - Show only LMStudio/Ollama models (Option 6)
- **OpenRouter toggle** - Hide/show marketplace models via config
- **Template system** - One-click apply 10 pre-built configurations (Option 7)
- **Smart recommendations** - Find free alternatives (Option 8)
- **Visual feedback** - Clear status messages

**Interactive selector features:**

- **Option 1-3**: Select BIG/MIDDLE/SMALL models
- **Option 4**: Configure reasoning settings (effort, max_tokens, exclude)
- **Option 5**: View current selections
- **Option 6**: Browse LOCAL models only (LMStudio, Ollama)
- **Option 7**: Use Template (pre-built configurations)
- **Option 8**: Get Recommendations (find free alternatives)
- **Option 9**: Save and apply configuration
- **Option 10**: Load saved mode
- **Option 11**: Save current as mode
- **Option 12**: List all modes

---

## üõ†Ô∏è Configuration

### Environment Variables

#### Required
```bash
PROVIDER_API_KEY="your-api-key"    # Your provider's API key

# Legacy (deprecated, but still works):
# OPENAI_API_KEY="your-api-key"
```

#### Security
```bash
PROXY_AUTH_KEY="exact-key"    # (Optional) Client must match this

# Legacy (deprecated, but still works):
# ANTHROPIC_API_KEY="exact-key"
```

#### Model Configuration
```bash
BIG_MODEL="openai/gpt-5"         # Claude Opus requests
MIDDLE_MODEL="openai/gpt-5"      # Claude Sonnet requests  
SMALL_MODEL="gpt-4o-mini"        # Claude Haiku requests
```

#### Reasoning Configuration
```bash
REASONING_EFFORT="high"          # low, medium, high
REASONING_MAX_TOKENS="8000"      # Optional: max tokens for reasoning
VERBOSITY="high"                 # Response detail
REASONING_EXCLUDE="false"        # Show/hide reasoning
```

**Reasoning Configuration Options:**

- **REASONING_EFFORT**: Controls reasoning intensity
  - `low`: ~20% of tokens for reasoning (faster)
  - `medium`: ~50% of tokens (balanced)
  - `high`: ~80% of tokens (best quality)
  - Empty/None: Disable reasoning

- **REASONING_MAX_TOKENS**: Fine-grained control (Anthropic/OpenRouter style)
  - Integer value (e.g., `2000`, `8000`, `16000`)
  - Directly specifies maximum tokens for reasoning
  - Works with Anthropic and OpenRouter models
  - Leave empty to use provider default

- **REASONING_EXCLUDE**: Control reasoning token visibility
  - `false`: Include reasoning tokens in response (default)
  - `true`: Use reasoning internally but exclude from response

- **VERBOSITY**: Response detail level
  - `high`: More detailed responses
  - `default` or empty: Standard detail

#### API Configuration
```bash
PROVIDER_BASE_URL="https://openrouter.ai/api/v1"  # Your provider's base URL

# Legacy (deprecated, but still works):
# OPENAI_BASE_URL="https://openrouter.ai/api/v1"
```

#### Selector Configuration
```bash
ENABLE_OPENROUTER_SELECTION="true"  # Show/hide OpenRouter models in selector
```

**Selector Configuration Options:**

- **ENABLE_OPENROUTER_SELECTION**: Control which models appear in interactive selector
  - `true`: Show all models (OpenRouter + local) (default)
  - `false`: Show local models only (LMStudio, Ollama)
  - Useful for local-only deployments or when you want to hide marketplace models
**Custom Headers:**

- `CUSTOM_HEADER_*` - Custom headers for API requests (e.g., `CUSTOM_HEADER_ACCEPT`, `CUSTOM_HEADER_AUTHORIZATION`)
  - Uncomment in `.env` file to enable custom headers

### Custom Headers Configuration

Add custom headers to your API requests by setting environment variables with the `CUSTOM_HEADER_` prefix:

```bash
# Uncomment to enable custom headers
# CUSTOM_HEADER_ACCEPT="application/jsonstream"
# CUSTOM_HEADER_CONTENT_TYPE="application/json"
# CUSTOM_HEADER_USER_AGENT="your-app/1.0.0"
# CUSTOM_HEADER_AUTHORIZATION="Bearer your-token"
# CUSTOM_HEADER_X_API_KEY="your-api-key"
# CUSTOM_HEADER_X_CLIENT_ID="your-client-id"
# CUSTOM_HEADER_X_CLIENT_VERSION="1.0.0"
# CUSTOM_HEADER_X_REQUEST_ID="unique-request-id"
# CUSTOM_HEADER_X_TRACE_ID="trace-123"
# CUSTOM_HEADER_X_SESSION_ID="session-456"
```

### Header Conversion Rules

Environment variables with the `CUSTOM_HEADER_` prefix are automatically converted to HTTP headers:

- Environment variable: `CUSTOM_HEADER_ACCEPT`
- HTTP Header: `ACCEPT`

- Environment variable: `CUSTOM_HEADER_X_API_KEY`
- HTTP Header: `X-API-KEY`

- Environment variable: `CUSTOM_HEADER_AUTHORIZATION`
- HTTP Header: `AUTHORIZATION`

### Supported Header Types

- **Content Type**: `ACCEPT`, `CONTENT-TYPE`
- **Authentication**: `AUTHORIZATION`, `X-API-KEY`
- **Client Identification**: `USER-AGENT`, `X-CLIENT-ID`, `X-CLIENT-VERSION`
- **Tracking**: `X-REQUEST-ID`, `X-TRACE-ID`, `X-SESSION-ID`

### Usage Example

```bash
# Basic configuration (use semantic names)
PROVIDER_API_KEY="sk-your-provider-api-key-here"
PROVIDER_BASE_URL="https://api.openai.com/v1"

# Enable custom headers (uncomment as needed)
CUSTOM_HEADER_ACCEPT="application/jsonstream"
CUSTOM_HEADER_CONTENT_TYPE="application/json"
CUSTOM_HEADER_USER_AGENT="my-app/1.0.0"
CUSTOM_HEADER_AUTHORIZATION="Bearer my-token"
```

The proxy will automatically include these headers in all API requests to the target LLM provider.

### Model Mapping

#### Hybrid Mode Configuration (Per-Model Routing)
```bash
# Enable per-model endpoints (set to "true" to enable)
ENABLE_BIG_ENDPOINT="true"
ENABLE_MIDDLE_ENDPOINT="true"
ENABLE_SMALL_ENDPOINT="true"

# Per-model endpoints (if enabled above)
BIG_ENDPOINT="http://localhost:11434/v1"
MIDDLE_ENDPOINT="https://openrouter.ai/api/v1"
SMALL_ENDPOINT="http://127.0.0.1:1234/v1"

# Per-model API keys (defaults to PROVIDER_API_KEY if not set)
BIG_API_KEY="sk-or-..."
MIDDLE_API_KEY=""
SMALL_API_KEY="dummy"
```

**Hybrid Mode Options:**

- **ENABLE_BIG_ENDPOINT**: Enable custom endpoint for BIG model (Claude Opus)
  - `true`: Use `BIG_ENDPOINT` and `BIG_API_KEY`
  - `false`: Use default `PROVIDER_BASE_URL` and `PROVIDER_API_KEY`

- **ENABLE_MIDDLE_ENDPOINT**: Enable custom endpoint for MIDDLE model (Claude Sonnet)
  - `true`: Use `MIDDLE_ENDPOINT` and `MIDDLE_API_KEY`
  - `false`: Use default `PROVIDER_BASE_URL` and `PROVIDER_API_KEY`

- **ENABLE_SMALL_ENDPOINT**: Enable custom endpoint for SMALL model (Claude Haiku)
  - `true`: Use `SMALL_ENDPOINT` and `SMALL_API_KEY`
  - `false`: Use default `PROVIDER_BASE_URL` and `PROVIDER_API_KEY`

- **Per-Model Endpoints**: Custom base URLs for different providers
  - Ollama: `http://localhost:11434/v1`
  - LMStudio: `http://127.0.0.1:1234/v1`
  - OpenRouter: `https://openrouter.ai/api/v1`
  - OpenAI: `https://api.openai.com/v1`
  - Azure OpenAI: `https://your-resource.openai.azure.com/openai/deployments/your-deployment`

- **Per-Model API Keys**: Optional per-model authentication
  - If not set, falls back to main `PROVIDER_API_KEY`
  - Useful for different API keys per provider
  - Local models can use dummy values

#### Server Settings
```bash
HOST="0.0.0.0"                   # Server host
PORT="8082"                      # Server port
LOG_LEVEL="info"                 # debug, info, warning, error
```

### Provider Examples

#### OpenAI
```bash
PROVIDER_API_KEY="sk-..."
PROVIDER_BASE_URL="https://api.openai.com/v1"
BIG_MODEL="gpt-4o"
MIDDLE_MODEL="gpt-4o"
SMALL_MODEL="gpt-4o-mini"
```

#### OpenRouter
```bash
PROVIDER_API_KEY="sk-or-..."
PROVIDER_BASE_URL="https://openrouter.ai/api/v1"
BIG_MODEL="openai/gpt-5"
REASONING_EFFORT="high"
```

#### Azure OpenAI
```bash
PROVIDER_API_KEY="your-azure-key"
PROVIDER_BASE_URL="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
BIG_MODEL="gpt-4"
MIDDLE_MODEL="gpt-4"
SMALL_MODEL="gpt-35-turbo"
```

#### Local Models (Ollama)
```bash
PROVIDER_API_KEY="dummy"  # Required but can be dummy
PROVIDER_BASE_URL="http://localhost:11434/v1"
BIG_MODEL="ollama/qwen2.5:72b"
MIDDLE_MODEL="ollama/llama3.1:70b"
SMALL_MODEL="ollama/llama3.1:8b"
REASONING_EFFORT="medium"
```

#### LMStudio
```bash
PROVIDER_API_KEY="dummy"  # Required but can be dummy
PROVIDER_BASE_URL="http://127.0.0.1:1234/v1"
BIG_MODEL="lmstudio/Meta-Llama-3.1-405B-Instruct"
MIDDLE_MODEL="lmstudio/Meta-Llama-3.1-70B-Instruct"
SMALL_MODEL="lmstudio/Meta-Llama-3.1-8B-Instruct"
```

### Hybrid Mode - Mix Local & Remote Models

**The project NOW supports TRUE hybrid deployments!** Route each model (BIG/MIDDLE/SMALL) to different providers simultaneously:

#### Hybrid Mode Examples

**Example 1: BIG=Local Ollama, MIDDLE=OpenRouter, SMALL=LMStudio**

```bash
# Main API config
PROVIDER_API_KEY="your-openrouter-key"
PROVIDER_BASE_URL="https://openrouter.ai/api/v1"

# BIG model - Local Ollama
ENABLE_BIG_ENDPOINT="true"
BIG_ENDPOINT="http://localhost:11434/v1"
BIG_API_KEY="dummy"
BIG_MODEL="ollama/qwen2.5:72b"

# MIDDLE model - OpenRouter
ENABLE_MIDDLE_ENDPOINT="false"  # Uses default PROVIDER_BASE_URL
MIDDLE_MODEL="openai/gpt-5"

# SMALL model - LMStudio
ENABLE_SMALL_ENDPOINT="true"
SMALL_ENDPOINT="http://127.0.0.1:1234/v1"
SMALL_API_KEY="dummy"
SMALL_MODEL="lmstudio/Meta-Llama-3.1-8B-Instruct"
```

**Example 2: All Different Providers**

```bash
# BIG - GPT-5 on OpenRouter
ENABLE_BIG_ENDPOINT="true"
BIG_ENDPOINT="https://openrouter.ai/api/v1"
BIG_API_KEY="sk-or-..."
BIG_MODEL="openai/gpt-5"

# MIDDLE - Azure OpenAI
ENABLE_MIDDLE_ENDPOINT="true"
MIDDLE_ENDPOINT="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
MIDDLE_API_KEY="your-azure-key"
MIDDLE_MODEL="gpt-4"
MIDDLE_AZURE_API_VERSION="2024-03-01-preview"

# SMALL - Local LMStudio
ENABLE_SMALL_ENDPOINT="true"
SMALL_ENDPOINT="http://127.0.0.1:1234/v1"
SMALL_API_KEY="dummy"
SMALL_MODEL="lmstudio/Meta-Llama-3.1-8B-Instruct"
```

**How It Works:**

1. **Enable per-model routing**: Set `ENABLE_BIG_ENDPOINT="true"`, etc.
2. **Configure endpoint**: Set `BIG_ENDPOINT`, `MIDDLE_ENDPOINT`, `SMALL_ENDPOINT`
3. **Optional API key**: Set per-model API keys or use main `OPENAI_API_KEY`
4. **Automatic routing**: Proxy automatically routes each request to the correct provider

**Benefits:**

- ‚úÖ **Cost optimization**: Use free local models for simple tasks
- ‚úÖ **Best-of-breed**: Pick the best model for each tier
- ‚úÖ **Failover**: If one provider is down, others still work
- ‚úÖ **Privacy**: Route sensitive requests to local models
- ‚úÖ **Performance**: Use local models for speed, cloud for complexity

### OpenRouter Reasoning Tokens

OpenRouter provides unified reasoning support across all providers. Claude Code Proxy implements full compatibility with OpenRouter's reasoning tokens specification.

**Reasoning Token Features:**

1. **Preserving Reasoning Blocks**: Automatically preserves reasoning tokens when `REASONING_EXCLUDE="false"`
   - Reasoning appears in `choices[].message.reasoning` field
   - Enables tool-calling workflows with continued reasoning
   - Maintains conversation integrity across multiple API calls

2. **Unified Control**: Single configuration works across all reasoning-capable models:
   - OpenAI (GPT-5, o1, o3)
   - Anthropic (Claude 3.7, 4.x)
   - Qwen, DeepSeek, xAI Grok, MiniMax, Kimi

3. **Dual Control Modes**:
   - **Effort-based**: `REASONING_EFFORT="high/medium/low"`
     - OpenAI-style proportional reasoning
     - ~20%/50%/80% of max_tokens for reasoning

   - **Token-based**: `REASONING_MAX_TOKENS="8000"`
     - Anthropic/OpenRouter-style direct control
     - Exact token budget for reasoning
     - Capped at 32,000 tokens max, 1,024 tokens min

**Example: Tool Calling with Reasoning**

```python
# First request with reasoning
response = client.chat.completions.create(
    model="openai/gpt-5",
    messages=[{"role": "user", "content": "What's 9.9 vs 9.11?"}],
    extra_body={
        "reasoning": {
            "effort": "high",
            "exclude": False
        }
    }
)

# Reasoning is preserved in response
reasoning = response.choices[0].message.reasoning
content = response.choices[0].message.content

# Pass reasoning back for continued reasoning
response2 = client.chat.completions.create(
    model="openai/gpt-5",
    messages=[
        {"role": "user", "content": f"Context: {reasoning}"},
        {"role": "user", "content": "Explain this comparison"}
    ],
    extra_body={
        "reasoning": {
            "effort": "high",
            "exclude": False
        }
    }
)
```

---

## üí° Usage Examples

### Interactive Model Selection

**Browse and configure models visually:**

```bash
# Step 1: Fetch latest models (optional, updates monthly)
python scripts/fetch_openrouter_models.py

# Step 2: Launch interactive selector
python scripts/select_model.py
```

**Features:**
- Browse 352 models with filtering
- Apply templates (one-click setup)
- Get recommendations (find free alternatives)
- Configure BIG/MIDDLE/SMALL models
- Set reasoning parameters
- Save as mode for later

### CLI Configuration

**Start with specific models:**
Test proxy functionality:

```bash
# Use GPT-5 with high reasoning
python start_proxy.py \
  --big-model openai/gpt-5 \
  --reasoning-effort high \
  --verbosity high

# Use local models
python start_proxy.py \
  --big-model ollama/qwen2.5:72b \
  --reasoning-effort medium
```

**Manage modes:**

```bash
# Save current config as mode
python start_proxy.py --save-mode production

# List all saved modes
python start_proxy.py --list-modes

# Load a mode
python start_proxy.py --load-mode production

# Delete a mode
python start_proxy.py --delete-mode production

# Save with parameters (shorthand)
python start_proxy.py \
  --big-model gpt-5 \
  --reasoning-effort high \
  --mode premium
```

### Programmatic Usage

**Apply a template:**

```python
from src.utils.templates import ModeTemplates

config = ModeTemplates.get_config("free-tier")
# Returns: {'BIG_MODEL': 'ollama/qwen2.5:72b', ...}
```

**Find free alternatives:**

```python
from src.utils.recommender import ModelRecommender

recommender = ModelRecommender()
alternatives = recommender.find_model_alternatives("openai/gpt-5", "free")

for alt in alternatives[:3]:
    print(f"{alt['model']['id']}: score={alt['score']}")
    for reason in alt['reasons'][:2]:
        print(f"  - {reason}")
```

---

## üîå API Reference

### Endpoints

#### POST /v1/messages

Main Claude API endpoint. Converts Claude requests to OpenAI format.

**Request:**
```json
{
  "model": "claude-3-5-sonnet-20241022",
  "max_tokens": 100,
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}
```

**Response:**
```json
{
  "id": "msg_...",
  "object": "message",
  "content": [...],
  "model": "openai/gpt-5"
}
```

### Model Mapping

| Claude Model | Maps To | Config Variable |
|--------------|---------|-----------------|
| `claude-3-opus-*` | BIG_MODEL | `BIG_MODEL` |
| `claude-3-sonnet-*` | MIDDLE_MODEL | `MIDDLE_MODEL` |
| `claude-3-5-sonnet-*` | MIDDLE_MODEL | `MIDDLE_MODEL` |
| `claude-3-haiku-*` | SMALL_MODEL | `SMALL_MODEL` |

---

## üìä Model Database

### Statistics

```
Total Models: 352
‚îú‚îÄ Local Models: 12 (at TOP)
‚îÇ  ‚îú‚îÄ LMStudio: 5 models
‚îÇ  ‚îî‚îÄ Ollama: 7 models
‚îÇ
‚îú‚îÄ OpenRouter Models: 340
‚îÇ  ‚îú‚îÄ Reasoning Support: 210 (62%)
‚îÇ  ‚îú‚îÄ Vision Support: 45 (13%)
‚îÇ  ‚îî‚îÄ Standard: 85 (25%)
‚îÇ
Pricing:
‚îú‚îÄ Free Models: 140 (40%)
‚îÇ  ‚îî‚îÄ All 12 local + 128 OpenRouter
‚îÇ
‚îî‚îÄ Paid Models: 212 (60%)
```

### Top Free Models (No API costs!)

1. **ollama/qwen2.5:72b** - 131K context, Reasoning ‚úì
2. **ollama/deepseek-v2.5:236b** - 131K context, Reasoning ‚úì
3. **ollama/llama3.1:70b** - 131K context
4. **ollama/mistral-nemo:12b** - 131K context
5. **lmstudio/Meta-Llama-3.1-405B-Instruct** - 131K context

### Top Reasoning Models

1. **openai/gpt-5** - Premium reasoning
2. **openai/o3-mini** - Fast reasoning
3. **qwen/qwen-2.5-thinking-32b** - Open source
4. **ollama/qwen2.5:72b** - Free local
5. **amazon/nova-premier-v1** - 1M context

---

## üìä Monitoring & Analytics

### Real-Time Dashboards

**Terminal Dashboard** (Live TUI with Rich):
```bash
# Enable terminal dashboard
python src/main.py --dashboard
# or
export ENABLE_DASHBOARD="true"
python src/main.py
```

**WebSocket Dashboard** (Browser-based):
```bash
# Start proxy normally
python src/main.py

# Access dashboard in browser
open http://localhost:8082/dashboard
```

Features:
- ‚ö° Real-time request waterfall
- üìà Performance metrics (latency, tokens/sec)
- üí∞ Cost tracking and estimation
- üîÑ Live routing visualization
- ‚ö†Ô∏è Error monitoring with recent errors
- ü§ñ Top model usage statistics

### Prompt Injection (Claude Code Context)

**Inject proxy stats into Claude Code prompts** - Give Claude visibility into proxy performance!

**Interactive Configuration**:
```bash
# Run the configurator
python configure_prompt_injection.py

# Select modules, size, and injection mode
# Generates commands for .zshrc and p10k
```

**Manual Configuration**:
```bash
# Enable prompt injection
export PROMPT_INJECTION_ENABLED="true"
export PROMPT_INJECTION_MODULES="status,performance,errors"
export PROMPT_INJECTION_SIZE="medium"  # large, medium, small
export PROMPT_INJECTION_MODE="auto"    # auto, always, manual, header
```

**Size Variants**:

- **Large** (~200-300 tokens): Multi-line boxes with full details
  ```
  ‚îå‚îÄ üîß Proxy Status ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Provider: OpenRouter | Proxy  ‚îÇ
  ‚îÇ BIG: openai/gpt-5             ‚îÇ
  ‚îÇ Reasoning: high | 8000 tokens ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ```

- **Medium** (~50-100 tokens): Single-line compact [RECOMMENDED]
  ```
  üîß OpenRouter | ü§ñ gpt-5 | üß† high | üîí
  ‚ö° 847req | 1234ms | 78t/s | $12.45
  ```

- **Small** (~20-40 tokens): Ultra-compact icons
  ```
  üåêüß† ‚ö°847r¬∑78t/s ‚úì99%
  ```

**Available Modules**:
- `status` - Provider, models, reasoning config
- `performance` - Requests, latency, speed, cost
- `errors` - Success rate, error types
- `models` - Top models and usage stats

**Injection Modes**:
- `auto` - Smart injection (tool calls, streaming) [RECOMMENDED]
- `always` - Inject on every request
- `header` - Compact header only
- `manual` - Programmatic control

**Powerlevel10k Integration**:
```zsh
# Add to ~/.p10k.zsh (see examples/p10k_integration.zsh)
function prompt_custom_proxy_status() {
  # Shows: üîß‚ö°‚úìü§ñ in your prompt
  ...
}
```

**Examples**: See `examples/PROMPT_INJECTION_EXAMPLES.md` for detailed scenarios.

### Analytics API

**Get Usage Summary**:
```bash
curl http://localhost:8082/api/analytics/summary?days=7
```

**Time-Series Data** (for charting):
```bash
curl http://localhost:8082/api/analytics/timeseries?days=7&interval=hour
```

**Cost Breakdown**:
```bash
curl http://localhost:8082/api/analytics/cost-breakdown?days=7
```

**Error Analytics**:
```bash
curl http://localhost:8082/api/analytics/errors?days=7
```

**Export Data**:
```bash
curl http://localhost:8082/api/analytics/export?days=30&format=csv -o analytics.csv
curl http://localhost:8082/api/analytics/export?days=30&format=json -o analytics.json
```

### Billing Integration

**Check Account Balances**:
```bash
curl http://localhost:8082/api/billing/balance
```

**Get Usage from Providers**:
```bash
curl http://localhost:8082/api/billing/usage?days=7
```

**Provider-Specific Data**:
```bash
curl http://localhost:8082/api/billing/provider/openrouter?days=7
```

### Model Benchmarking

**Run Benchmark**:
```bash
curl -X POST "http://localhost:8082/api/benchmarks/run?model_name=gpt-4o&iterations=3"
```

**Compare Models**:
```bash
curl -X POST "http://localhost:8082/api/benchmarks/compare?models=gpt-4o&models=gpt-4o-mini"
```

**Get Benchmark Results**:
```bash
curl http://localhost:8082/api/benchmarks/results
```

**View Specific Result**:
```bash
curl http://localhost:8082/api/benchmarks/results/benchmark_gpt-4o_20250119_123456.json
```

### Multi-User Management

**Create User**:
```bash
curl -X POST http://localhost:8082/api/users \
  -H "Content-Type: application/json" \
  -d '{
    "username": "john",
    "email": "john@example.com",
    "quota_requests": 1000,
    "quota_tokens": 1000000,
    "quota_cost": 10.0
  }'
```

**Get Current User**:
```bash
curl http://localhost:8082/api/users/me \
  -H "x-api-key: sk-..."
```

**Check Quota**:
```bash
curl http://localhost:8082/api/users/me/quota \
  -H "x-api-key: sk-..."
```

**View Usage History**:
```bash
curl http://localhost:8082/api/users/me/usage?days=7 \
  -H "x-api-key: sk-..."
```

### Configuration

**Enable Usage Tracking**:
```bash
# .env
TRACK_USAGE="true"
```

**Enable Terminal Dashboard**:
```bash
# .env
ENABLE_DASHBOARD="true"
DASHBOARD_LAYOUT="default"  # default, compact, detailed
DASHBOARD_REFRESH="0.5"     # seconds between updates
DASHBOARD_WATERFALL_SIZE="20"  # number of requests to show
```

**Compact Logger Mode**:
```bash
# Reduce console noise when dashboard is active
COMPACT_LOGGER="true"
```

---

## üê≥ Docker Deployment

### Quick Start with Docker

**Using Docker Compose (Recommended):**
```bash
# 1. Configure environment
cp .env.example .env
# Edit .env with your settings

# 2. Build and start
docker-compose up --build

# 3. Use with Claude Code
export ANTHROPIC_BASE_URL=http://localhost:8082
claude "Hello world"
```

**Manual Docker Build:**
```bash
# Build image
docker build -t claude-code-proxy .

# Run container
docker run -p 8082:8082 --env-file .env claude-code-proxy

# Or with environment variables (use semantic names)
docker run -p 8082:8082 \
  -e PROVIDER_API_KEY="your-key" \
  -e PROVIDER_BASE_URL="https://openrouter.ai/api/v1" \
  -e BIG_MODEL="openai/gpt-5" \
  claude-code-proxy
```

### Docker Configuration

**Environment Variables:**
- Mount `.env` file: `-v $(pwd)/.env:/app/.env`
- Or pass directly: `-e OPENAI_API_KEY="..." -e BIG_MODEL="..."`

**Persistent Configuration:**
```bash
# Mount modes.json for persistent configuration
docker run -p 8082:8082 \
  -v $(pwd)/.env:/app/.env \
  -v $(pwd)/modes.json:/app/modes.json \
  claude-code-proxy
```

**Dashboard with Docker:**
```bash
# Run with dashboard modules
docker run -p 8082:8082 \
  -e DASHBOARD_MODULES="performance:dense,activity:sparse" \
  --env-file .env \
  claude-code-proxy
```

### Production Docker Setup

**docker-compose.prod.yml:**
```yaml
services:
  proxy:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8082:8082"
    environment:
      - PROVIDER_API_KEY=${PROVIDER_API_KEY}
      - PROVIDER_BASE_URL=${PROVIDER_BASE_URL}
      - BIG_MODEL=${BIG_MODEL}
      - REASONING_EFFORT=high
    volumes:
      - ./modes.json:/app/modes.json
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

---

## üõ†Ô∏è Development

### Project Structure

```
claude-code-proxy/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_manager.py    # Model mapping
‚îÇ   ‚îú‚îÄ‚îÄ conversion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ request_converter.py # Claude ‚Üí OpenAI conversion
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ endpoints.py        # API routes
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modes.py            # Configuration modes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates.py        # 10 pre-built templates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recommender.py      # Smart recommendations
‚îÇ   ‚îî‚îÄ‚îÄ client.py               # OpenAI client
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ fetch_openrouter_models.py  # Model database fetcher
‚îÇ   ‚îî‚îÄ‚îÄ select_model.py             # Interactive selector
‚îú‚îÄ‚îÄ models/                     # Model databases
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_models.json  # 352 models
‚îÇ   ‚îî‚îÄ‚îÄ openrouter_models.csv   # Spreadsheet format
‚îú‚îÄ‚îÄ .env.example                # Config template
‚îú‚îÄ‚îÄ .gitignore                  # Git ignore rules
‚îú‚îÄ‚îÄ start_proxy.py              # Startup script
‚îî‚îÄ‚îÄ README.md                   # This file
```

### Development Setup

```bash
# Install dependencies
uv sync

# Run server in development
uv run python start_proxy.py

# Format code
uv run black src/
uv run isort src/

# Type checking
uv run mypy src/

# Run tests
uv run pytest
```

### Adding New Features

**Add a new model source:**

1. Add models to `scripts/fetch_openrouter_models.py`
2. Add endpoint in `get_local_providers()`
3. Update model selector in `scripts/select_model.py`

**Add a new template:**

1. Edit `src/utils/templates.py`
2. Add template to `TEMPLATES` dict
3. Include config, description, requirements

**Extend recommender:**

1. Edit `src/utils/recommender.py`
2. Add new recommendation algorithm
3. Update scoring logic

---

## üìà Performance

- **Async/await** for high concurrency
- **Connection pooling** for efficiency
- **Streaming support** for real-time responses
- **Configurable timeouts** and retries
- **Smart error handling** with detailed logging
- **Automatic model detection** for reasoning parameters

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Ways to contribute:

- üêõ Report bugs
- üí° Suggest features
- üìö Improve documentation
- üîß Submit pull requests
- ‚≠ê Star the repository

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [Anthropic](https://anthropic.com) for Claude Code
- [OpenAI](https://openai.com) for the API
- [OpenRouter](https://openrouter.ai) for model aggregation
- [FastAPI](https://fastapi.tiangolo.com) for the web framework
- [Ollama](https://ollama.com) for local model serving
- [LMStudio](https://lmstudio.ai) for the GUI

---

## üìû Support

- üìñ [Documentation](https://github.com/yourusername/claude-code-proxy/wiki)
- üêõ [Issues](https://github.com/yourusername/claude-code-proxy/issues)
- üí¨ [Discussions](https://github.com/yourusername/claude-code-proxy/discussions)

---

## üéõÔ∏è Dashboard System

### ‚úÖ **FULLY IMPLEMENTED** - Modular Dashboard System

**Interactive Configuration Tool:**
- ‚úÖ `configure_dashboard.py` - Interactive module selection with previews
- ‚úÖ Dense vs Sparse display modes for all modules
- ‚úÖ 1-4 module selection with live previews
- ‚úÖ Command generation for Claude Code and .zshrc integration
- ‚úÖ Startup script generation with executable permissions

**Dashboard Manager:**
- ‚úÖ `src/dashboard/dashboard_manager.py` - Orchestrates multiple modules
- ‚úÖ Environment variable configuration (`DASHBOARD_MODULES`)
- ‚úÖ Rich layout system (1-4 modules in grid/split layouts)
- ‚úÖ Live updating dashboard with configurable refresh rates
- ‚úÖ Plain text fallback when Rich library unavailable

**Live Dashboard Interface:**
- ‚úÖ `src/dashboard/live_dashboard.py` - Replaces terminal output
- ‚úÖ Real-time API monitoring with Rich interface
- ‚úÖ Integration with existing request logger
- ‚úÖ Signal handling (Ctrl+C graceful shutdown)
- ‚úÖ Full-screen dashboard mode

**5 Complete Dashboard Modules:**

1. **‚úÖ Performance Monitor** (`src/dashboard/modules/performance_monitor.py`)
   - Dense: Full performance panel with progress bars, context usage, thinking tokens
   - Sparse: Single line with key metrics (duration, tokens, cost, speed)
   - Real-time session tracking with color coding
   - Cost estimation and efficiency calculations

2. **‚úÖ Activity Feed** (`src/dashboard/modules/activity_feed.py`)
   - Dense: Multi-session request history with status icons
   - Sparse: Compact status summary with request counts
   - Request pairing (start/complete/error tracking)
   - Model routing display and performance metrics

3. **‚úÖ Routing Visualizer** (`src/dashboard/modules/routing_visualizer.py`)
   - Dense: Visual model routing flow with ASCII art
   - Sparse: Compact routing info with token flow
   - Context and output token visualization
   - Performance and cost analysis per routing decision

4. **‚úÖ Analytics Panel** (`src/dashboard/modules/analytics_panel.py`)
   - Dense: Comprehensive analytics (requests, cost, performance, model usage)
   - Sparse: Key metrics summary line
   - Success rate calculation and performance extremes
   - Hot model tracking and usage patterns

5. **‚úÖ Request Waterfall** (`src/dashboard/modules/request_waterfall.py`)
   - Dense: Detailed request lifecycle with timing breakdown
   - Sparse: Compact lifecycle summary
   - Phase-by-phase timing (Parse‚ÜíRoute‚ÜíThink‚ÜíSend‚ÜíWait‚ÜíRecv‚ÜíDone)
   - Real-time progress for active requests

**Base Module System:**
- ‚úÖ `src/dashboard/modules/base_module.py` - Common functionality
- ‚úÖ Request history management with configurable limits
- ‚úÖ Token formatting, cost estimation, progress bars
- ‚úÖ Rich and plain text rendering support
- ‚úÖ Active/completed request tracking

### üéØ **DASHBOARD USAGE**

**Quick Setup:**
```bash
# 1. Configure your dashboard interactively
python configure_dashboard.py

# 2. Select modules (e.g., performance:dense,activity:sparse)
# 3. Copy generated commands to .zshrc or run directly:
export DASHBOARD_MODULES="performance:dense,activity:sparse"

# 4. Start live dashboard
python -m src.dashboard.live_dashboard
```

**Example Configurations:**
```bash
# Performance monitoring only
DASHBOARD_MODULES="performance:dense"

# Multi-module setup
DASHBOARD_MODULES="performance:sparse,activity:sparse,analytics:dense"

# Full dashboard (4 modules)
DASHBOARD_MODULES="performance:dense,activity:dense,routing:sparse,waterfall:sparse"
```

**Generated Commands:**
- ‚úÖ Environment variable export
- ‚úÖ Claude Code integration instructions
- ‚úÖ .zshrc aliases for easy access
- ‚úÖ Executable startup script (`start_dashboard.sh`)

### üìä **DASHBOARD PREVIEW EXAMPLES**

**Dense Mode Example (Performance Monitor):**
```
‚îå‚îÄ API Performance Monitor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîµ Session abc123 | anthropic/claude-3.5-sonnet‚Üíopenai/gpt-4o ‚îÇ
‚îÇ ‚ö° 15.8s | üìä CTX: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 43.7k/200k (22%) | 82 tok/s    ‚îÇ
‚îÇ üß† THINK: ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 920 tokens | üí∞ $0.0234 estimated       ‚îÇ
‚îÇ üì§ OUT: ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.3k/16k | üåä STREAMING | 3msg + SYS     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Sparse Mode Example (Activity Feed):**
```
üîµabc123‚ÜíOK üü¢def456‚ÜíOK üî¥ghi789‚ÜíERR üîµjkl012‚ÜíOK | 4req 3.2s avg
```

### üîß **DASHBOARD INTEGRATION STATUS**

**‚úÖ PHASE 1 COMPLETE - Standalone Dashboard:**
- Interactive configuration tool with previews
- All 5 dashboard modules (dense + sparse modes)
- Dashboard manager with layout system
- Live dashboard with real-time updates
- Request logger integration
- Command generation for setup
- Environment variable configuration
- Rich formatting with fallback to plain text
- Signal handling and graceful shutdown

**‚úÖ TESTED:**
- Module rendering in both modes
- Configuration parsing and validation
- Layout generation for 1-4 modules
- Request data flow from logger to modules
- Cost estimation and token formatting
- Progress bar generation
- Model name formatting and provider detection

**üîÑ PHASE 2 COMPLETED - Integrated Proxy Dashboard:**
- ‚úÖ **Terminal dashboard** - Live Rich-based TUI with real-time updates
- ‚úÖ **Edge-based module positioning** - Modules on top/bottom/left/right edges
- ‚úÖ **Central waterfall display** - Live request flow visualization
- ‚úÖ **WebSocket dashboard** - Browser-based real-time dashboard at `/dashboard`
- ‚úÖ **Historical data persistence** - Analytics API with time-series data
- ‚úÖ **Advanced analytics** - Cost breakdown, error analysis, trend tracking
- ‚úÖ **Dashboard hooks system** - Real-time data flow from proxy to dashboards

**‚úÖ PHASE 3 COMPLETED - Advanced Features:**
- ‚úÖ **Prompt injection system** - Inject proxy stats into Claude Code prompts
- ‚úÖ **Multi-size variants** - Large, medium, small (for token optimization)
- ‚úÖ **Interactive configurator** - `configure_prompt_injection.py` with live previews
- ‚úÖ **Powerlevel10k integration** - Show proxy status in your shell prompt
- ‚úÖ **Real-time billing integration** - Provider API integration (OpenRouter, OpenAI, Anthropic)
- ‚úÖ **Automated model benchmarking** - Standardized tests, performance comparison
- ‚úÖ **Multi-user authentication** - User management with API keys and quotas
- ‚úÖ **Database backend** - Enhanced SQLite with CSV/JSON export

**üîÑ PHASE 4 IN DEVELOPMENT:**
- [ ] Moveable modules in terminal dashboard - Drag modules between edges
- [ ] Layout persistence - Save/load dashboard arrangements
- [ ] Resize panels - Dynamic module sizing
- [ ] Custom dashboard themes and color schemes
- [ ] Module creation API for extensions

---

## üéâ Complete Features Summary

### ‚úÖ **CORE PROXY FEATURES (FULLY WORKING)**
- **352 models** from multiple sources (OpenRouter, Ollama, LMStudio)
- **Comprehensive reasoning support** (OpenAI, Anthropic, all providers)
- **Dual reasoning control** (effort + max_tokens)
- **OpenRouter reasoning tokens** with block preservation
- **Multi-provider support** (OpenAI, Azure, OpenRouter, local)
- **Function calling** & streaming support
- **Claude API compatibility** with full `/v1/messages` endpoint
- **Smart model mapping** (BIG/MIDDLE/SMALL configuration)
- **Hybrid mode** - Mix local and remote models simultaneously
- **Per-model routing** - Each model to different provider

### ‚úÖ **CONFIGURATION SYSTEM (FULLY WORKING)**
- **10 pre-built templates** for quick setup
- **Smart recommendations** with cost optimization
- **99 configuration modes** for flexibility
- **Interactive selector** with beautiful UI + filtering
- **Local-only mode** (hide/show OpenRouter models)
- **CLI configuration** for automation
- **Environment variable management**
- **Mode saving/loading system**

### ‚úÖ **DASHBOARD SYSTEM (FULLY WORKING)**
- **Terminal dashboard** - Live Rich-based TUI with real-time updates
- **WebSocket dashboard** - Browser-based dashboard at `/dashboard`
- **5 complete dashboard modules** with dense/sparse modes
- **Interactive configuration tool** with live previews
- **Modular architecture** (1-4 modules, user selectable)
- **Real-time monitoring** with Rich terminal interface
- **Request lifecycle tracking** with detailed analytics
- **Cost and performance monitoring**
- **Session-based color coding**
- **Command generation** for integration

### ‚úÖ **MODEL DATABASE (FULLY WORKING)**
- **352 total models** with complete metadata
- **140 free models** (local + OpenRouter free tier)
- **Reasoning support detection** (210+ models)
- **Vision support tracking** (45+ models)
- **Pricing information** per 1M tokens
- **Context window limits** for all models
- **Provider endpoint mapping**

### ‚úÖ **LOGGING SYSTEM (FULLY WORKING)**
- **Rich colored terminal output** with session colors
- **Comprehensive request logging** (start/complete/error)
- **Token usage visualization** with progress bars
- **Performance metrics** (tokens/sec, latency)
- **Cost estimation** with real-time tracking
- **Context window monitoring** with percentage usage
- **Thinking token tracking** for reasoning models

### ‚úÖ **MONITORING & ANALYTICS (FULLY WORKING)**
- **Historical data persistence** - Analytics API with time-series data
- **Cost breakdown** - By model, provider, and time period
- **Error analytics** - Success rates, error types, trends
- **Data export** - CSV and JSON format export
- **Real-time billing integration** - OpenRouter, OpenAI, Anthropic APIs
- **Account balance tracking** - Multi-provider support
- **Automated benchmarking** - Standardized performance tests
- **Model comparison** - Side-by-side benchmarking

### ‚úÖ **PROMPT INJECTION SYSTEM (FULLY WORKING)**
- **Claude Code integration** - Inject proxy stats into prompts
- **3 size variants** - Large (~250 tokens), Medium (~80 tokens), Small (~30 tokens)
- **4 dashboard modules** - Status, performance, errors, models
- **Interactive configurator** - `configure_prompt_injection.py` with previews
- **Smart injection modes** - Auto, always, manual, header
- **Powerlevel10k integration** - Show status in shell prompt
- **Nerd Font support** - Beautiful icons: üîß ‚ö° ‚ö†Ô∏è ‚úì ü§ñ üß†
- **Environment variable config** - Easy setup and customization

### ‚úÖ **MULTI-USER SYSTEM (FULLY WORKING)**
- **User management** - Create users with API keys
- **Quota enforcement** - Per-user limits (requests, tokens, cost)
- **Usage tracking** - Daily usage per user
- **Authentication** - Secure API key validation
- **User analytics** - Per-user statistics and history
- **Database storage** - Persistent user data

### üîÑ **FUTURE ENHANCEMENTS**

The following features are planned for future releases:

**Terminal Dashboard Enhancements:**
- [ ] **Drag-and-drop module positioning** - Interactively move modules between edges
- [ ] **Dynamic panel resizing** - Adjust module sizes on the fly
- [ ] **Auto-hide inactive modules** - Clean interface when modules not needed
- [ ] **Focus mode** - Expand waterfall to full screen
- [ ] **Layout persistence** - Save and load custom dashboard arrangements

**Theming & Customization:**
- [ ] **Custom color schemes** - Dark, light, and custom themes
- [ ] **Module creation API** - Build your own dashboard modules
- [ ] **Plugin system** - Third-party extensions for dashboards
- [ ] **Configurable refresh rates** - Per-module update frequencies

**Advanced Analytics:**
- [ ] **Trend prediction** - ML-based cost and usage forecasting
- [ ] **Anomaly detection** - Automatic alerts for unusual patterns
- [ ] **Cost optimization recommendations** - Automated suggestions
- [ ] **A/B testing framework** - Compare model performance

**Enterprise Features:**
- [ ] **Team analytics** - Multi-user usage insights
- [ ] **Budget management** - Set and enforce spending limits
- [ ] **Audit logging** - Comprehensive request audit trails
- [ ] **SSO integration** - Enterprise authentication

### ‚ö†Ô∏è **KNOWN LIMITATIONS**
- **Terminal dashboard** - Limited to 4 modules in standalone mode
- **Cost estimates** - Approximate, not from real-time billing (except OpenRouter)
- **WebSocket dashboard** - Basic implementation, no advanced filtering yet
- **Benchmarking** - Background tasks only, no scheduling yet

### üîí **SECURITY NOTES**
- **No hardcoded secrets** - All API keys via environment variables
- **Gitignore protection** - Sensitive files excluded from version control
- **Clean repository** - No development artifacts or temporary files

### üß™ **TESTING STATUS**
- ‚úÖ **Core proxy functionality** - Fully tested with multiple providers
- ‚úÖ **Model selection and configuration** - Tested with all templates
- ‚úÖ **Dashboard modules** - All 5 modules tested in both modes
- ‚úÖ **Request logging integration** - Tested with live API calls
- ‚úÖ **Configuration management** - Mode saving/loading tested
- ‚úÖ **Interactive tools** - Selector and configurator tested
- üîÑ **Load testing** - In progress for high-volume scenarios
- üîÑ **Edge case handling** - Ongoing testing for error conditions

---

## üìã **AUDIT CHECKLIST**

### ‚úÖ **IMPLEMENTED AND WORKING**
- [x] Claude API proxy with full compatibility
- [x] Multi-provider support (OpenAI, Azure, OpenRouter, local)
- [x] Reasoning token support across all providers
- [x] 352 model database with metadata
- [x] Interactive model selector with filtering
- [x] 10 pre-built configuration templates
- [x] 99 configuration mode slots
- [x] Smart model recommendations
- [x] Hybrid deployment support
- [x] Rich terminal logging with colors
- [x] Complete dashboard system (5 modules)
- [x] Dashboard configuration tool
- [x] Live dashboard interface
- [x] Request lifecycle tracking
- [x] Cost and performance monitoring
- [x] Session-based analytics

### ‚úÖ **NEWLY IMPLEMENTED** (Latest Update)
- [x] **WebSocket dashboard** for browser - Real-time dashboard at `/dashboard`
- [x] **Historical data persistence** - Analytics API with time-series data
- [x] **Advanced analytics** - Cost breakdown, error analysis, trend tracking
- [x] **Real-time billing integration** - Provider API integration (OpenRouter, OpenAI, Anthropic)
- [x] **Automated model benchmarking** - Standardized tests, performance comparison
- [x] **Multi-user authentication** - User management with API keys and quotas
- [x] **Database backend** - Enhanced SQLite with export capabilities

### üîÑ **IN DEVELOPMENT**
- [ ] Custom dashboard themes
- [ ] Module creation API
- [ ] Web-based configuration interface (currently API-only)

---

<div align="center">

**Made with ‚ù§Ô∏è for the developer community**

[‚≠ê Star on GitHub](https://github.com/yourusername/claude-code-proxy)

</div>
