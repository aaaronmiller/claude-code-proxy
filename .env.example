# Required: Your OpenAI API key
OPENAI_API_KEY="sk-your-openai-api-key-here"

# Optional: Expected Anthropic API key for client validation
# If set, clients must provide this exact API key to access the proxy
ANTHROPIC_API_KEY="your-expected-anthropic-api-key"

# Optional: OpenAI API base URL (default: https://api.openai.com/v1)
# You can change this to use other providers like Azure OpenAI, local models, etc.
OPENAI_BASE_URL="https://api.openai.com/v1"

# Optional: Model mappings (BIG and SMALL models)
# BIG_MODEL="gpt-4o"
# Used for Claude opus requests
# MIDDLE_MODEL="gpt-4o"
# Used for Claude sonnet requests
# sSMALL_MODEL="gpt-4o-mini"
# Used for Claude haiku requests

# ═══════════════════════════════════════════════════════════════════════════════
# REASONING CONFIGURATION - Advanced reasoning for OpenAI, Anthropic, and Gemini
# ═══════════════════════════════════════════════════════════════════════════════

# Global reasoning effort level for OpenAI o-series models (o1, o3, o4-mini, gpt-5)
# Valid values: "low", "medium", "high"
# Default: not set (reasoning disabled unless explicitly enabled)
# This setting applies to all reasoning-capable models unless overridden by model suffix
# Example: REASONING_EFFORT="high"
REASONING_EFFORT=""

# Maximum tokens for reasoning/thinking (Anthropic Claude and Google Gemini)
# Anthropic Claude (3.7, 4.x): Valid range 1024-16000 tokens
# Google Gemini (2.5-flash): Valid range 0-24576 tokens
# Default: not set (uses provider default)
# Example: REASONING_MAX_TOKENS="8000"
REASONING_MAX_TOKENS=""

# Whether to exclude reasoning tokens from the response
# Set to "true" to hide reasoning tokens in the response (OpenAI only)
# Default: "false" (include reasoning tokens)
REASONING_EXCLUDE="false"

# Verbosity setting for responses (affects detail level)
# Provider-specific setting for response verbosity
# Default: not set
# Example: VERBOSITY="high"
VERBOSITY=""

# ═══════════════════════════════════════════════════════════════════════════════
# USAGE TRACKING & ANALYTICS (Optional)
# ═══════════════════════════════════════════════════════════════════════════════

# Enable persistent API usage tracking (opt-in feature)
# When enabled, stores request metadata in SQLite for analytics
# Does NOT store message content - only metadata (tokens, cost, performance)
# Data stored: model, tokens, duration, cost estimates, request counts
# Privacy: All tracking is local, no data sent anywhere
# Default: "false" (disabled)
TRACK_USAGE="false"

# Usage database location
# Default: "usage_tracking.db" in project root
# USAGE_DB_PATH="usage_tracking.db"

# Enable compact single-line logging format
# Uses emojis and sophisticated color coding for terminal output
# Alternative to the default multi-line format
# Default: "false" (uses standard logger)
USE_COMPACT_LOGGER="false"

# ═══════════════════════════════════════════════════════════════════════════════
# PER-MODEL REASONING OVERRIDES (Optional)
# ═══════════════════════════════════════════════════════════════════════════════

# Override reasoning effort for specific model sizes
# These override the global REASONING_EFFORT setting for each model
# Valid values: "low", "medium", "high"
# Only applies to reasoning-capable models (OpenAI o-series)

# BIG model reasoning override (Claude Opus → configured BIG_MODEL)
# Example: BIG_MODEL_REASONING="high"
BIG_MODEL_REASONING=""

# MIDDLE model reasoning override (Claude Sonnet → configured MIDDLE_MODEL)
# Example: MIDDLE_MODEL_REASONING="medium"
MIDDLE_MODEL_REASONING=""

# SMALL model reasoning override (Claude Haiku → configured SMALL_MODEL)
# Example: SMALL_MODEL_REASONING="low"
SMALL_MODEL_REASONING=""

# ═══════════════════════════════════════════════════════════════════════════════
# REASONING EXAMPLES - Model name suffix notation
# ═══════════════════════════════════════════════════════════════════════════════

# You can also specify reasoning parameters directly in model names using suffix notation:
#
# OpenAI o-series (effort levels OR arbitrary token budgets):
#   - "o4-mini:low"      → Low reasoning effort
#   - "o4-mini:medium"   → Medium reasoning effort
#   - "o4-mini:high"     → High reasoning effort
#   - "o4-mini:50000"    → 50,000 thinking tokens (arbitrary budget)
#   - "o4-mini:350000"   → 350,000 thinking tokens (for 400k context)
#   - "o4-mini:100k"     → 102,400 thinking tokens (k-notation)
#
# Anthropic Claude (thinking tokens):
#   - "claude-opus-4-20250514:1k"    → 1024 thinking tokens
#   - "claude-opus-4-20250514:4k"    → 4096 thinking tokens
#   - "claude-opus-4-20250514:8k"    → 8192 thinking tokens
#   - "claude-opus-4-20250514:16k"   → 16384 thinking tokens
#   - "claude-opus-4-20250514:8000"  → 8000 thinking tokens (exact number)
#
# Google Gemini (thinking budget):
#   - "gemini-2.5-flash-preview-04-17:4k"     → 4096 thinking budget
#   - "gemini-2.5-flash-preview-04-17:16000"  → 16000 thinking budget
#
# Suffix notation overrides environment variable defaults
# Terminal output shows: model routing, endpoint, thinking tokens used vs budget

# Optional: Enable/disable OpenRouter model selection in interactive selector
# Set to "false" to hide OpenRouter marketplace models (only show local models)
# Default: "true" (show all models including OpenRouter)
# Example: ENABLE_OPENROUTER_SELECTION="false"
ENABLE_OPENROUTER_SELECTION="true"

# ═══════════════════════════════════════════════════════════════════════════════
# HYBRID MODE - Optional per-model routing (mix local and remote models!)
# ═══════════════════════════════════════════════════════════════════════════════

# Enable per-model routing for BIG model (Claude Opus)
# Set to "true" to enable custom endpoint for BIG model
# Example: ENABLE_BIG_ENDPOINT="true"
ENABLE_BIG_ENDPOINT="false"

# BIG model endpoint URL (if enabled above)
# Example: BIG_ENDPOINT="http://localhost:11434/v1"
BIG_ENDPOINT="https://api.openai.com/v1"

# BIG model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: BIG_API_KEY="sk-or-..."
BIG_API_KEY=""

# Enable per-model routing for MIDDLE model (Claude Sonnet)
# Set to "true" to enable custom endpoint for MIDDLE model
# Example: ENABLE_MIDDLE_ENDPOINT="true"
ENABLE_MIDDLE_ENDPOINT="false"

# MIDDLE model endpoint URL (if enabled above)
# Example: MIDDLE_ENDPOINT="https://openrouter.ai/api/v1"
MIDDLE_ENDPOINT="https://api.openai.com/v1"

# MIDDLE model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: MIDDLE_API_KEY="sk-or-..."
MIDDLE_API_KEY=""

# Enable per-model routing for SMALL model (Claude Haiku)
# Set to "true" to enable custom endpoint for SMALL model
# Example: ENABLE_SMALL_ENDPOINT="true"
ENABLE_SMALL_ENDPOINT="false"

# SMALL model endpoint URL (if enabled above)
# Example: SMALL_ENDPOINT="http://127.0.0.1:1234/v1"
SMALL_ENDPOINT="https://api.openai.com/v1"

# SMALL model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: SMALL_API_KEY="sk-or-..."
SMALL_API_KEY=""

# Optional: Server settings
HOST="0.0.0.0"
PORT="8082"
LOG_LEVEL="INFO"  
# DEBUG, INFO, WARNING, ERROR, CRITICAL

# ═══════════════════════════════════════════════════════════════════════════════
# WEB UI CONFIGURATION (Optional)
# ═══════════════════════════════════════════════════════════════════════════════

# The Web UI is automatically available at http://localhost:8082/ or http://localhost:8082/config
# Features:
# - Visual configuration editor (no restart required!)
# - Profile management (save/load/delete different configs)
# - Model browser with search and filtering
# - Real-time monitoring and statistics
# - Usage analytics (when TRACK_USAGE is enabled)
#
# No additional configuration needed - just access http://localhost:8082 in your browser!

# ═══════════════════════════════════════════════════════════════════════════════
# TERMINAL OUTPUT CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

# Log style: "rich" (colored with progress bars), "plain" (no colors), "compact" (minimal)
# Default: "rich"
LOG_STYLE="rich"

# Show token counts in logs (input/output/thinking tokens)
# Default: "true"
SHOW_TOKEN_COUNTS="true"

# Show performance metrics (tokens/sec, latency)
# Default: "true"
SHOW_PERFORMANCE="true"

# Color scheme: "auto" (detect terminal), "dark", "light", "none" (disable colors)
# Default: "auto"
COLOR_SCHEME="auto"

# Optional: Performance settings  
MAX_TOKENS_LIMIT="4096"
# Minimum tokens limit for requests (to avoid errors with thinking model)
MIN_TOKENS_LIMIT="4096"
REQUEST_TIMEOUT="90"
MAX_RETRIES="2"

# Examples for other providers:

# For Azure OpenAI (recommended if OpenAI is not available in your region):
# OPENAI_API_KEY="your-azure-api-key"
# OPENAI_BASE_URL="https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name"
# AZURE_API_VERSION="2024-03-01-preview"
# BIG_MODEL="gpt-4"
# MIDDLE_MODEL="gpt-4"
# SMALL_MODEL="gpt-35-turbo"

# For local models (like Ollama):
# OPENAI_API_KEY="dummy-key"  # Required but can be any value for local models
# OPENAI_BASE_URL="http://localhost:11434/v1"
# BIG_MODEL="llama3.1:70b"
# MIDDLE_MODEL="llama3.1:70b"
# SMALL_MODEL="llama3.1:8b"

# Note: If you get "unsupported_country_region_territory" errors,
# consider using Azure OpenAI or a local model setup instead.


# Custom Headers Configuration
# Format: HEADER_KEY=header_value
# These headers will be automatically included in API requests
# Uncomment the lines below to use custom headers:
# CUSTOM_HEADER_ACCEPT="application/jsonstream"
# CUSTOM_HEADER_CONTENT_TYPE="application/json"
# CUSTOM_HEADER_USER_AGENT="node-fetch"
# CUSTOM_HEADER_HOST="example.com"
# CUSTOM_HEADER_AUTHORIZATION="Bearer your-token"
# CUSTOM_HEADER_X_API_KEY="your-api-key"
# CUSTOM_HEADER_X_CLIENT_ID="your-client-id"
