# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# Рџа№ИЈ  TEMPLATE FILE - DO NOT SOURCE DIRECTLY Рџа№ИЈ
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# This is a template configuration file with placeholder values.
# Copy this file to .env and replace placeholders with your actual values:
#
#   cp .env.example .env
#   # Then edit .env with your real API keys
#
# IMPORTANT: If your shell auto-loads .env files, make sure it EXCLUDES .env.example
# to prevent placeholder values from overwriting your real configuration!
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# SEMANTIC VARIABLE NAMES (RECOMMENDED - USE THESE)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# These variable names clearly represent what they actually do.
# Use these instead of the legacy names below for clarity and correctness.

# Required: Your API key for your backend provider
# Works with ANY OpenAI-compatible provider:
# - OpenRouter: sk-or-v1-YOUR-KEY-HERE
# - OpenAI: sk-YOUR-KEY-HERE
# - Azure: your-azure-key
# - Local models: any value (or "dummy-key")
PROVIDER_API_KEY="sk-your-provider-api-key-here"

# Optional: API base URL for your backend provider (default: https://api.openai.com/v1)
# Common examples:
# - OpenRouter: https://openrouter.ai/api/v1
# - OpenAI: https://api.openai.com/v1
# - Azure: https://your-resource.openai.azure.com/openai/deployments/your-deployment-name
# - Local (Ollama): http://localhost:11434/v1
# - Local (LM Studio): http://localhost:1234/v1
PROVIDER_BASE_URL="https://api.openai.com/v1"

# Optional: Authentication key for proxy clients
# If set, clients must provide this exact key to access the proxy
# This is NOT for Anthropic's API - it's for securing YOUR proxy
PROXY_AUTH_KEY="your-proxy-auth-key"

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# LEGACY VARIABLE NAMES (DEPRECATED - DO NOT USE)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# These variable names are misleading and deprecated. They still work for backward
# compatibility but will show deprecation warnings. Use the semantic names above.
#
# OPENAI_API_KEY Рєњ Use PROVIDER_API_KEY (this key is for ANY provider, not just OpenAI)
# OPENAI_BASE_URL Рєњ Use PROVIDER_BASE_URL
# ANTHROPIC_API_KEY Рєњ Use PROXY_AUTH_KEY (this is for proxy auth, NOT Anthropic's API)
#
# DO NOT SET THESE - USE THE SEMANTIC NAMES ABOVE INSTEAD
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Optional: Model mappings (BIG and SMALL models)
# BIG_MODEL="gpt-4o"
# Used for Claude opus requests
# MIDDLE_MODEL="gpt-4o"
# Used for Claude sonnet requests
# sSMALL_MODEL="gpt-4o-mini"
# Used for Claude haiku requests

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# REASONING CONFIGURATION - Advanced reasoning for OpenAI, Anthropic, and Gemini
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Global reasoning effort level for OpenAI o-series models (o1, o3, o4-mini, gpt-5)
# Valid values: "low", "medium", "high"
# Default: not set (reasoning disabled unless explicitly enabled)
# This setting applies to all reasoning-capable models unless overridden by model suffix
# Example: REASONING_EFFORT="high"
REASONING_EFFORT=""

# Maximum tokens for reasoning/thinking (Anthropic Claude and Google Gemini)
# Anthropic Claude (3.7, 4.x): Valid range 1024-16000 tokens
# Google Gemini (2.5-flash): Valid range 0-24576 tokens
# Default: not set (uses provider default)
# Example: REASONING_MAX_TOKENS="8000"
REASONING_MAX_TOKENS=""

# Whether to exclude reasoning tokens from the response
# Set to "true" to hide reasoning tokens in the response (OpenAI only)
# Default: "false" (include reasoning tokens)
REASONING_EXCLUDE="false"

# Verbosity setting for responses (affects detail level)
# Provider-specific setting for response verbosity
# Default: not set
# Example: VERBOSITY="high"
VERBOSITY=""

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# USAGE TRACKING & ANALYTICS (Optional)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Enable persistent API usage tracking (opt-in feature)
# When enabled, stores request metadata in SQLite for analytics
# Does NOT store message content - only metadata (tokens, cost, performance)
# Data stored: model, tokens, duration, cost estimates, request counts
# Privacy: All tracking is local, no data sent anywhere
# Default: "false" (disabled)
TRACK_USAGE="false"

# Usage database location
# Default: "usage_tracking.db" in project root
# USAGE_DB_PATH="usage_tracking.db"

# Enable compact single-line logging format
# Uses emojis and sophisticated color coding for terminal output
# Alternative to the default multi-line format
# Default: "false" (uses standard logger)
COMPACT_LOGGER="false"

# Enable live terminal dashboard
# Shows real-time metrics, active requests, and performance stats
# Default: "false"
ENABLE_DASHBOARD="false"

# Dashboard layout style
# Options: "default", "compact", "detailed"
# Default: "default"
DASHBOARD_LAYOUT="default"

# Dashboard refresh rate in seconds
# Default: "0.5"
DASHBOARD_REFRESH="0.5"

# Number of recent requests to show in waterfall view
# Default: "20"
DASHBOARD_WATERFALL_SIZE="20"

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# PER-MODEL REASONING OVERRIDES (Optional)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Override reasoning effort for specific model sizes
# These override the global REASONING_EFFORT setting for each model
# Valid values: "low", "medium", "high"
# Only applies to reasoning-capable models (OpenAI o-series)

# BIG model reasoning override (Claude Opus Рєњ configured BIG_MODEL)
# Example: BIG_MODEL_REASONING="high"
BIG_MODEL_REASONING=""

# MIDDLE model reasoning override (Claude Sonnet Рєњ configured MIDDLE_MODEL)
# Example: MIDDLE_MODEL_REASONING="medium"
MIDDLE_MODEL_REASONING=""

# SMALL model reasoning override (Claude Haiku Рєњ configured SMALL_MODEL)
# Example: SMALL_MODEL_REASONING="low"
SMALL_MODEL_REASONING=""

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# REASONING EXAMPLES - Model name suffix notation
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# You can also specify reasoning parameters directly in model names using suffix notation:
#
# OpenAI o-series (effort levels OR arbitrary token budgets):
#   - "o4-mini:low"      Рєњ Low reasoning effort
#   - "o4-mini:medium"   Рєњ Medium reasoning effort
#   - "o4-mini:high"     Рєњ High reasoning effort
#   - "o4-mini:50000"    Рєњ 50,000 thinking tokens (arbitrary budget)
#   - "o4-mini:350000"   Рєњ 350,000 thinking tokens (for 400k context)
#   - "o4-mini:100k"     Рєњ 102,400 thinking tokens (k-notation)
#
# Anthropic Claude (thinking tokens):
#   - "claude-opus-4-20250514:1k"    Рєњ 1024 thinking tokens
#   - "claude-opus-4-20250514:4k"    Рєњ 4096 thinking tokens
#   - "claude-opus-4-20250514:8k"    Рєњ 8192 thinking tokens
#   - "claude-opus-4-20250514:16k"   Рєњ 16384 thinking tokens
#   - "claude-opus-4-20250514:8000"  Рєњ 8000 thinking tokens (exact number)
#
# Google Gemini (thinking budget):
#   - "gemini-2.5-flash-preview-04-17:4k"     Рєњ 4096 thinking budget
#   - "gemini-2.5-flash-preview-04-17:16000"  Рєњ 16000 thinking budget
#
# Suffix notation overrides environment variable defaults
# Terminal output shows: model routing, endpoint, thinking tokens used vs budget

# Optional: Enable/disable OpenRouter model selection in interactive selector
# Set to "false" to hide OpenRouter marketplace models (only show local models)
# Default: "true" (show all models including OpenRouter)
# Example: ENABLE_OPENROUTER_SELECTION="false"
ENABLE_OPENROUTER_SELECTION="true"

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# HYBRID MODE - Optional per-model routing (mix local and remote models!)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Enable per-model routing for BIG model (Claude Opus)
# Set to "true" to enable custom endpoint for BIG model
# Example: ENABLE_BIG_ENDPOINT="true"
ENABLE_BIG_ENDPOINT="false"

# BIG model endpoint URL (if enabled above)
# Example: BIG_ENDPOINT="http://localhost:11434/v1"
BIG_ENDPOINT="https://api.openai.com/v1"

# BIG model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: BIG_API_KEY="sk-or-..."
BIG_API_KEY=""

# Enable per-model routing for MIDDLE model (Claude Sonnet)
# Set to "true" to enable custom endpoint for MIDDLE model
# Example: ENABLE_MIDDLE_ENDPOINT="true"
ENABLE_MIDDLE_ENDPOINT="false"

# MIDDLE model endpoint URL (if enabled above)
# Example: MIDDLE_ENDPOINT="https://openrouter.ai/api/v1"
MIDDLE_ENDPOINT="https://api.openai.com/v1"

# MIDDLE model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: MIDDLE_API_KEY="sk-or-..."
MIDDLE_API_KEY=""

# Enable per-model routing for SMALL model (Claude Haiku)
# Set to "true" to enable custom endpoint for SMALL model
# Example: ENABLE_SMALL_ENDPOINT="true"
ENABLE_SMALL_ENDPOINT="false"

# SMALL model endpoint URL (if enabled above)
# Example: SMALL_ENDPOINT="http://127.0.0.1:1234/v1"
SMALL_ENDPOINT="https://api.openai.com/v1"

# SMALL model API key (if enabled above, defaults to OPENAI_API_KEY if not set)
# Example: SMALL_API_KEY="sk-or-..."
SMALL_API_KEY=""

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# CUSTOM SYSTEM PROMPTS (Optional)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Override system prompts for specific model tiers
# Useful for customizing behavior per model size

# Enable custom prompts per model
ENABLE_CUSTOM_BIG_PROMPT="false"
ENABLE_CUSTOM_MIDDLE_PROMPT="false"
ENABLE_CUSTOM_SMALL_PROMPT="false"

# System prompt from file (takes precedence if both file and inline are set)
# BIG_SYSTEM_PROMPT_FILE="/path/to/big_prompt.txt"
# MIDDLE_SYSTEM_PROMPT_FILE="/path/to/middle_prompt.txt"
# SMALL_SYSTEM_PROMPT_FILE="/path/to/small_prompt.txt"

# System prompt inline (used if no file is specified)
# BIG_SYSTEM_PROMPT="You are a helpful assistant specialized in..."
# MIDDLE_SYSTEM_PROMPT="You are a helpful assistant..."
# SMALL_SYSTEM_PROMPT="You are a helpful assistant..."

# Optional: Server settings
HOST="0.0.0.0"
PORT="8082"
LOG_LEVEL="INFO"
# DEBUG, INFO, WARNING, ERROR, CRITICAL

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# WEB UI CONFIGURATION (Optional)
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# The Web UI is automatically available at http://localhost:8082/ or http://localhost:8082/config
# Features:
# - Visual configuration editor (no restart required!)
# - Profile management (save/load/delete different configs)
# - Model browser with search and filtering
# - Real-time monitoring and statistics
# - Usage analytics (when TRACK_USAGE is enabled)
#
# No additional configuration needed - just access http://localhost:8082 in your browser!

# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ
# TERMINAL OUTPUT CONFIGURATION
# РЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљРЋљ

# Terminal display mode
# Options: "minimal", "normal", "detailed", "debug"
# Default: "detailed"
TERMINAL_DISPLAY_MODE="detailed"

# Show workspace/project name in terminal output
# Default: "true"
TERMINAL_SHOW_WORKSPACE="true"

# Show context window percentage
# Default: "true"
TERMINAL_SHOW_CONTEXT_PCT="true"

# Show task type icons (­ЪДа REASON, ­ЪћД TOOLS, ­ЪЊЮ TEXT, ­Ъќ╝№ИЈ IMAGE)
# Default: "true"
TERMINAL_SHOW_TASK_TYPE="true"

# Show tokens per second (performance metric)
# Default: "true"
TERMINAL_SHOW_SPEED="true"

# Show cost estimates per request
# Default: "true"
TERMINAL_SHOW_COST="true"

# Enable color-coded request durations (fast=green, medium=yellow, slow=red)
# Default: "true"
TERMINAL_SHOW_DURATION_COLORS="true"

# Color scheme for terminal output
# Options: "auto", "vibrant", "subtle", "mono"
# Default: "auto"
TERMINAL_COLOR_SCHEME="auto"

# Enable session color differentiation (different color per Claude Code session)
# Default: "true"
TERMINAL_SESSION_COLORS="true"

# Log style: "rich" (colored with progress bars), "plain" (no colors), "compact" (minimal)
# Default: "rich"
LOG_STYLE="rich"

# Show token counts in logs (input/output/thinking tokens)
# Default: "true"
SHOW_TOKEN_COUNTS="true"

# Show performance metrics (tokens/sec, latency)
# Default: "true"
SHOW_PERFORMANCE="true"

# Color scheme: "auto" (detect terminal), "dark", "light", "none" (disable colors)
# Default: "auto"
COLOR_SCHEME="auto"

# Optional: Performance settings  
MAX_TOKENS_LIMIT="4096"
# Minimum tokens limit for requests (to avoid errors with thinking model)
MIN_TOKENS_LIMIT="4096"
REQUEST_TIMEOUT="90"
MAX_RETRIES="2"

# Examples for other providers:

# For Azure OpenAI (recommended if OpenAI is not available in your region):
# OPENAI_API_KEY="your-azure-api-key"
# OPENAI_BASE_URL="https://your-resource-name.openai.azure.com/openai/deployments/your-deployment-name"
# AZURE_API_VERSION="2024-03-01-preview"
# BIG_MODEL="gpt-4"
# MIDDLE_MODEL="gpt-4"
# SMALL_MODEL="gpt-35-turbo"

# For local models (like Ollama):
# OPENAI_API_KEY="dummy-key"  # Required but can be any value for local models
# OPENAI_BASE_URL="http://localhost:11434/v1"
# BIG_MODEL="llama3.1:70b"
# MIDDLE_MODEL="llama3.1:70b"
# SMALL_MODEL="llama3.1:8b"

# Note: If you get "unsupported_country_region_territory" errors,
# consider using Azure OpenAI or a local model setup instead.


# Custom Headers Configuration
# Format: HEADER_KEY=header_value
# These headers will be automatically included in API requests
# Uncomment the lines below to use custom headers:
# CUSTOM_HEADER_ACCEPT="application/jsonstream"
# CUSTOM_HEADER_CONTENT_TYPE="application/json"
# CUSTOM_HEADER_USER_AGENT="node-fetch"
# CUSTOM_HEADER_HOST="example.com"
# CUSTOM_HEADER_AUTHORIZATION="Bearer your-token"
# CUSTOM_HEADER_X_API_KEY="your-api-key"
# CUSTOM_HEADER_X_CLIENT_ID="your-client-id"
